{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Centroids Classification Via PyTorch and Scikit-Learn\n",
    "\n",
    "- Using PyTorch implementation of ResNet50 to get embeddings\n",
    "- Using Scikit-Learn k-means clustering to get centroids of each class\n",
    "- Using Scikit-Learn's knn clasifier to fit the centroids and for inference\n",
    "- Using Matplotlib to visualise the centroids on a 2D space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfile import ZipFile\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ResNet50 with default weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50_Weights.IMAGENET1K_V2\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "print(ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights so that web app can load from S3 next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../model/resnet50_default_embedder.pth\"\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(*list(resnet50().children())[:-1])\n",
    "loaded_state_dict = torch.load(model_name)\n",
    "model.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define image transformation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image_bytes: bytes):\n",
    "    # Default unchanged mean and std\n",
    "    # mean = [0.5, 0.5, 0.5]\n",
    "    # std = [0.5, 0.5, 0.5]\n",
    "\n",
    "    # Imagenet mean and std\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    transform_output = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "    return transform_output(image).float().unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 2048, 1, 1])\n",
      "torch.Size([2048])\n",
      "tensor([0.0390, 0.0876, 0.0322,  ..., 0.1154, 0.0867, 0.0668],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/mock_s3_train/tench/n01440764_1009.JPEG\", \"rb\") as f:\n",
    "    image_bytes = f.read()\n",
    "\n",
    "img = transform_image(image_bytes)\n",
    "img_features = model(img)\n",
    "\n",
    "print(img.shape)\n",
    "print(img_features.shape)\n",
    "print(img_features.squeeze().shape)\n",
    "print(img_features.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Centroids via K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get image features for all images in folder and stack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2048])\n"
     ]
    }
   ],
   "source": [
    "model.eval() #Important\n",
    "folder_name = \"../data/mock_s3_train/tench\"\n",
    "all_img_features_list = []\n",
    "for image_filename in os.listdir(folder_name):\n",
    "    with open(f\"{folder_name}/{image_filename}\", \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    img = transform_image(image_bytes)\n",
    "    img_features = model(img).squeeze()\n",
    "    all_img_features_list.append(img_features)\n",
    "\n",
    "all_img_features = torch.stack(all_img_features_list)\n",
    "print(all_img_features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define k-means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Variables to consider saving in S3 together with PyTorch ResNet50 model\n",
    "num_centroids = 30\n",
    "random_state = 2023\n",
    "kmeans = KMeans(n_clusters=num_centroids, random_state=random_state, init='k-means++', n_init=1)\n",
    "kmeans.fit(all_img_features.detach().numpy())\n",
    "print(kmeans.cluster_centers_.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the above into a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(folder_name: str, num_centroids: int, random_state: int):\n",
    "    # Below section to be modified to retrieve images from S3\n",
    "    all_img_features_list = []\n",
    "    for image_filename in os.listdir(f\"{folder_name}\"):\n",
    "        with open(f\"{folder_name}/{image_filename}\", \"rb\") as f:\n",
    "            image_bytes = f.read()\n",
    "        img = transform_image(image_bytes)\n",
    "        img_features = model(img).squeeze()\n",
    "        all_img_features_list.append(img_features)\n",
    "\n",
    "    all_img_features = torch.stack(all_img_features_list)\n",
    "    kmeans = KMeans(n_clusters=num_centroids, random_state=random_state, init='k-means++', n_init=1)\n",
    "    kmeans.fit(all_img_features.detach().numpy())\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get centroids for both folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2048)\n",
      "(30, 2048)\n",
      "[0.04882131 0.         0.03448014 ... 0.00260307 0.03191869 0.00851635]\n",
      "[0.0344649 0.        0.        ... 0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "all_centroids_dict = {}\n",
    "\n",
    "for folder_name in [\"tench\", \"coucal\"]:\n",
    "    all_centroids_dict[folder_name] = get_centroids(f\"../data/mock_s3_train/{folder_name}\", num_centroids, random_state)\n",
    "\n",
    "print(all_centroids_dict[\"tench\"].shape)\n",
    "print(all_centroids_dict[\"coucal\"].shape)\n",
    "print(all_centroids_dict[\"tench\"][0])\n",
    "print(all_centroids_dict[\"coucal\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save centroids as joblib file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/all_centroids_dict.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(all_centroids_dict, \"../model/all_centroids_dict.joblib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load centroids from joblib file to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2048)\n",
      "(30, 2048)\n",
      "[0.04882131 0.         0.03448014 ... 0.00260307 0.03191869 0.00851635]\n",
      "[0.0344649 0.        0.        ... 0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "all_centroids_dict = joblib.load(\"../model/all_centroids_dict.joblib\")\n",
    "\n",
    "print(all_centroids_dict[\"tench\"].shape)\n",
    "print(all_centroids_dict[\"coucal\"].shape)\n",
    "print(all_centroids_dict[\"tench\"][0])\n",
    "print(all_centroids_dict[\"coucal\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KNN Classifier on Centroids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define knn classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbours = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=num_neighbours)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit knn to centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tench\n",
      "coucal\n",
      "(60, 2048)\n",
      "(60,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for key in all_centroids_dict.keys():\n",
    "    print(key)\n",
    "    X.append(all_centroids_dict[key])\n",
    "    y.append([key] * len(all_centroids_dict[key]))\n",
    "\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tench']\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test_img_feature = img_features.detach().numpy().reshape(1, -1)\n",
    "print(knn.predict(test_img_feature))\n",
    "print(knn.predict_proba(test_img_feature))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save classifier as joblib file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/knn.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn, \"../model/knn.joblib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load classifier from joblib file to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tench']\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "knn = joblib.load(\"../model/knn.joblib\")\n",
    "print(knn.predict(test_img_feature))\n",
    "print(knn.predict_proba(test_img_feature))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Batch Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying single inference function that is already done from `helper_model.py` to cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model_dict: dict, image_bytes: bytes):\n",
    "    class_names = model_dict[\"classifier\"].classes_\n",
    "    img = transform_image(image_bytes=image_bytes)\n",
    "    model_dict[\"feature_extractor\"].eval()\n",
    "    with torch.no_grad():\n",
    "        img_features = model_dict[\"feature_extractor\"](img)\n",
    "        img_features = img_features.detach().numpy().reshape(1, -1)\n",
    "        pred_probas = model_dict[\"classifier\"].predict_proba(img_features)[0]\n",
    "        predicted_classes = {\n",
    "            predicted_class: confidence\n",
    "            for (predicted_class, confidence) in zip(class_names, pred_probas)\n",
    "        }\n",
    "        predicted_classes = {\n",
    "            key: value\n",
    "            for (key, value) in sorted(\n",
    "                predicted_classes.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "        }\n",
    "        top_predictions = {\n",
    "            key: value for (key, value) in list(predicted_classes.items())[:5]\n",
    "        }\n",
    "    return top_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick testing of function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tench': 1.0, 'coucal': 0.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction({\n",
    "    \"feature_extractor\": model,\n",
    "    \"classifier\": knn\n",
    "}, image_bytes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code logic for looping through zip file and performing inference for image files that have no issues. Mini evaluation will be done to categorise predictions as correct or wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible params for function\n",
    "test_batch_evaluation_zip = \"../data/for_zipping/test_batch_evaluation.zip\"\n",
    "base_folder_to_save = \"../data/for_zipping\"\n",
    "model_dict = {\n",
    "    \"feature_extractor\": model,\n",
    "    \"classifier\": knn\n",
    "}\n",
    "top_k = 1\n",
    "\n",
    "# Code logic for batch evaluation function\n",
    "overall_prediction = {\n",
    "    \"top_k\": top_k,\n",
    "    \"correct\": {},\n",
    "    \"wrong\": {},\n",
    "    \"skipped\": {}\n",
    "}\n",
    "with ZipFile(test_batch_evaluation_zip, \"r\") as zip_ref:\n",
    "    for filename in zip_ref.namelist():\n",
    "        if any(filename.lower().endswith(prefix) for prefix in [\".png\", \".jpg\", \".jpeg\"]):\n",
    "            try:\n",
    "                with zip_ref.open(filename) as f:\n",
    "                    image_bytes = f.read()\n",
    "            except Exception as err:\n",
    "                print(err)\n",
    "                image_bytes = None\n",
    "                overall_prediction[\"skipped\"][filename] = \"File could not be read.\"\n",
    "\n",
    "            if image_bytes is not None:\n",
    "                try:\n",
    "                    predictions = get_prediction(model_dict, image_bytes)\n",
    "\n",
    "                    #  Change logic to check top 5 predictions instead of top 1 prediction\n",
    "                    #  where there are more classes\n",
    "                    if any(filename.lower().startswith(suffix) for suffix in list(predictions.keys())[:top_k]):\n",
    "                        overall_prediction[\"correct\"][filename] = predictions\n",
    "                    else:\n",
    "                        overall_prediction[\"wrong\"][filename] = predictions\n",
    "                except Exception as err:\n",
    "                    print(err)\n",
    "                    overall_prediction[\"skipped\"][filename] = \"Prediction failed.\"\n",
    "        else:\n",
    "            overall_prediction[\"skipped\"][filename] = \"File extension not supported.\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIew results summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No. Correct: 56\n",
      " No. Correct (Coucal): 28\n",
      " No. Correct (Tench): 28\n",
      "\n",
      " No. Wrong: 0\n",
      " No. Wrong (Coucal): 0\n",
      " No. Wrong (Tench): 0\n",
      "\n",
      " No. Skipped: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\" No. Correct: {len(overall_prediction['correct'])}\")\n",
    "print(f\" No. Correct (Coucal): {len([filename for filename in overall_prediction['correct'] if filename.startswith('coucal')])}\")\n",
    "print(f\" No. Correct (Tench): {len([filename for filename in overall_prediction['correct'] if filename.startswith('tench')])}\")\n",
    "print()\n",
    "print(f\" No. Wrong: {len(overall_prediction['wrong'])}\")\n",
    "print(f\" No. Wrong (Coucal): {len([filename for filename in overall_prediction['wrong'] if filename.startswith('coucal')])}\")\n",
    "print(f\" No. Wrong (Tench): {len([filename for filename in overall_prediction['wrong'] if filename.startswith('tench')])}\")\n",
    "print()\n",
    "print(f\" No. Skipped: {len(overall_prediction['skipped'])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which tench images got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = 'tench'\n",
    "for filename in [filename for filename in overall_prediction['wrong'] if filename.startswith(class_name)]:\n",
    "    print(filename, overall_prediction['wrong'][filename])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making dict into json for upload into S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_prediction_json = json.dumps(overall_prediction, indent=4)\n",
    "with open(f\"{base_folder_to_save}/overall_prediction.json\", \"w\") as f:\n",
    "    f.write(overall_prediction_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Centroids via 2D PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit PCA for all centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_X = pca.fit_transform(X)\n",
    "pca_X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot PCA points on 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "for label in np.unique(y):\n",
    "    idx = np.where(y == label)\n",
    "    ax.scatter(pca_X[idx, 0], pca_X[idx, 1], label=label)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-ils-be",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
